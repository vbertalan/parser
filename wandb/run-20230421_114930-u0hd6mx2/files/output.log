
  0%|          | 0/6179 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.




















































































































  8%|▊         | 501/6179 [03:56<41:25,  2.28it/s]




















































































































 16%|█▌        | 997/6179 [07:49<36:44,  2.35it/s]























































































































 24%|██▍       | 1501/6179 [11:49<36:25,  2.14it/s]





















































































































 32%|███▏      | 2002/6179 [15:44<30:08,  2.31it/s]





















































































































 40%|████      | 2497/6179 [19:39<27:35,  2.22it/s]





















































































































 49%|████▊     | 2999/6179 [23:35<24:31,  2.16it/s]























































































































 57%|█████▋    | 3501/6179 [27:34<21:47,  2.05it/s]






















































































































 65%|██████▍   | 4001/6179 [31:31<22:36,  1.61it/s]






















































































































 73%|███████▎  | 4500/6179 [35:28<12:45,  2.19it/s]





















































































































 81%|████████  | 4999/6179 [39:23<08:31,  2.31it/s]





















































































































 89%|████████▉ | 5501/6179 [43:19<04:56,  2.28it/s]






















































































































 97%|█████████▋| 6001/6179 [47:16<01:24,  2.10it/s]








































100%|██████████| 6179/6179 [48:38<00:00,  2.87it/s]
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 6179/6179 [48:38<00:00,  2.12it/s]
Saving model checkpoint to ./Hadoop-Transformer
{'train_runtime': 2927.6703, 'train_samples_per_second': 135.056, 'train_steps_per_second': 2.111, 'train_loss': 0.7749586461874868, 'epoch': 1.0}
Configuration saved in ./Hadoop-Transformer/config.json
Model weights saved in ./Hadoop-Transformer/pytorch_model.bin