{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira etapa - receber logs\n",
    "log1 = \"make deploy http://www.google.com\"\n",
    "log2 = \"error http://www.google.com\"\n",
    "log3 = \"make deploy http://www.microsoft.com\"\n",
    "log4 = \"error http://www.microsoft.com\"\n",
    "log5 = \"make deploy https://www.apple.com\"\n",
    "log6 = \"error https://www.apple.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segunda etapa - identificar palavras em inglês? (facultativo - ver outro ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(type(embeddings[0]))\\nprint(len(embeddings[0]))\\nprint(embeddings[0].ndim)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Terceira etapa - vetorizar via Transformers\n",
    "\n",
    "## Importa Sentence Transformers e compila o modelo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "## Junta as frases\n",
    "sentences = [log1, log2, log3, log4, log5, log6]\n",
    "\n",
    "## Faz encode nas frases\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "## Printa os embeddings\n",
    "'''\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "'''\n",
    "\n",
    "## Verifica características dos embeddings\n",
    "'''\n",
    "print(type(embeddings[0]))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings[0].ndim)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusteriza com UMAP e DBSCAN\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "# Referência DBSCAN: https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\n",
    "# Referênia UMAP: https://umap-learn.readthedocs.io/en/latest/\n",
    "\n",
    "## Parametriza clusterizador HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2,\n",
    "              min_samples=1,\n",
    "              metric='euclidean',\n",
    "              allow_single_cluster=False,                  \n",
    "              cluster_selection_method='eom')\n",
    "\n",
    "## Parametriza redutor UMAP\n",
    "reducer = umap.UMAP(n_neighbors=2, n_components=1, spread=0.5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "## Roda clustering com UMAP\n",
    "umap_data = reducer.fit_transform(embeddings)\n",
    "hdb = clusterer.fit(umap_data)\n",
    "## Roda clustering sem UMAP\n",
    "#hdb = clusterer.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgglomerativeClustering()\n",
      "[1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "## Clusteriza com SKLearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "sk_clusterer = AgglomerativeClustering().fit(embeddings)\n",
    "print(sk_clusterer)\n",
    "print(sk_clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1\n",
      "['make deploy http://www.google.com', 'make deploy http://www.microsoft.com', 'make deploy https://www.apple.com']\n",
      "\n",
      "Cluster  2\n",
      "['error http://www.google.com', 'error http://www.microsoft.com', 'error https://www.apple.com']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Clusteriza com K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Clusteriza com K-Means\n",
    "num_clusters = 2\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for _ in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Imprime labels dos clusters\n",
    "clusterer.labels_\n",
    "\n",
    "## Acha número máximo de clusters\n",
    "#clusterer.labels_.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira hipótese: usar Prefix Tree para identificar campos comuns e diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do LCS é 29\n"
     ]
    }
   ],
   "source": [
    "## Segunda hipótese: usar LCS, depois verificar tokens comuns - implementação com programação dinâmica de LCS - complexidade O(n*m)\n",
    "\n",
    "def lcs(x, y):\n",
    "  # Acha tamanho das strings\n",
    "  m = len(x)\n",
    "  n = len(y)\n",
    "\n",
    "  # Declara array para guardar os valores da programação dinâmica\n",
    "  vals = [[None]*(n + 1) for _ in range(m + 1)]\n",
    "\n",
    "  # Compara strings em ordem bottom-up\n",
    "  for i in range(m + 1):\n",
    "    for j in range(n + 1):\n",
    "      if i == 0 or j == 0 :\n",
    "        vals[i][j] = 0\n",
    "      elif x[i-1] == y[j-1]:\n",
    "        vals[i][j] = vals[i-1][j-1]+1\n",
    "      else:\n",
    "        vals[i][j] = max(vals[i-1][j], vals[i][j-1])\n",
    "\n",
    "  # vals[m][n] vai conter o tamanho do LCS entre x[0..n-1] e y[0..m-1]\n",
    "  return vals[m][n]\n",
    "\n",
    "# Teste de LCs\n",
    "print(\"O tamanho do LCS é\", lcs(log1, log3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste usando split\n",
      "['017-09-26', '12:40:15,', 'INFO', 'impl.FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home/data3/current', '30ms']\n",
      "Teste usando NLTK\n",
      "['017-09-26', '12:40:15', ',', 'INFO', 'impl.FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home/data3/current', '30ms']\n",
      "Teste usando Gensim\n",
      "['INFO', 'impl', 'FsDatasetImpl', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP', 'on', 'home', 'data', 'current', 'ms']\n",
      "Teste usando Spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['017', '-', '09', '-', '26', '12:40:15', ',', 'INFO', 'impl', '.', 'FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home', '/', 'data3', '/', 'current', '30ms']\n"
     ]
    }
   ],
   "source": [
    "## Métodos de tokenização\n",
    "\n",
    "teste = \"017-09-26 12:40:15, INFO impl.FsDatasetImpl - Time taken to scan block pool BP-805143380 on /home/data3/current 30ms\"\n",
    "\n",
    "## Teste usando split\n",
    "print (\"Teste usando split\")\n",
    "print(teste.split())\n",
    "\n",
    "## Teste usando NLTK\n",
    "print (\"Teste usando NLTK\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(teste))\n",
    "\n",
    "## Teste usando Gensim\n",
    "print (\"Teste usando Gensim\")\n",
    "from gensim.utils import tokenize\n",
    "print(list(tokenize(teste)))\n",
    "\n",
    "## Teste usando Spacy\n",
    "print (\"Teste usando Spacy\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(teste)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'deploy', 'http', ':', '//www.google.com']\n",
      "['make', 'deploy', 'http', ':', '//www.microsoft.com']\n",
      "{'make': 'STATIC', 'deploy': 'STATIC', 'http': 'STATIC', ':': 'STATIC', '//www.google.com': 'VARIABLE'}\n"
     ]
    }
   ],
   "source": [
    "## Compara duas strings para tokens similares - não leva em conta posição\n",
    "\n",
    "## Usando NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compare_strings_as_list(x,y):\n",
    "    tokens_x = word_tokenize(x)\n",
    "    tokens_y = word_tokenize(y)\n",
    "    compared = []\n",
    "    has_token = False\n",
    "\n",
    "    for i in tokens_x:\n",
    "        for j in tokens_y:\n",
    "            if i == j:\n",
    "                has_token = True\n",
    "        if has_token == True:\n",
    "            compared.append(i)\n",
    "        else:\n",
    "            compared.append(\"VARIABLE\")\n",
    "        has_token = False\n",
    "    \n",
    "    return compared\n",
    "\n",
    "#print(word_tokenize(log1))\n",
    "#print(word_tokenize(log3))\n",
    "#print(compare_strings_as_list(log1, log3))\n",
    "\n",
    "def compare_strings_as_dict(x,y):\n",
    "    tokens_x = word_tokenize(x)\n",
    "    tokens_y = word_tokenize(y)\n",
    "    compared = {}\n",
    "    has_token = False\n",
    "\n",
    "    for i in tokens_x:\n",
    "        for j in tokens_y:\n",
    "            if i == j:\n",
    "                has_token = True\n",
    "        compared[i] = \"STATIC\" if has_token == True else \"VARIABLE\"\n",
    "        has_token = False\n",
    "\n",
    "    return compared\n",
    "\n",
    "print(word_tokenize(log1))\n",
    "print(word_tokenize(log3))\n",
    "print(compare_strings_as_dict(log1, log3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
