{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira etapa - receber logs\n",
    "log1 = \"make deploy http://www.google.com\"\n",
    "log2 = \"error http://www.google.com\"\n",
    "log3 = \"make deploy http://www.microsoft.com\"\n",
    "log4 = \"error http://www.microsoft.com\"\n",
    "log5 = \"make deploy https://www.apple.com\"\n",
    "log6 = \"error https://www.apple.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segunda etapa - identificar palavras em inglês? (facultativo - ver outro ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(type(embeddings[0]))\\nprint(len(embeddings[0]))\\nprint(embeddings[0].ndim)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Terceira etapa - vetorizar via Transformers\n",
    "\n",
    "## Importa Sentence Transformers e compila o modelo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "## Junta as frases\n",
    "sentences = [log1, log2, log3, log4, log5, log6]\n",
    "\n",
    "## Faz encode nas frases\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "## Printa os embeddings\n",
    "'''\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "'''\n",
    "\n",
    "## Verifica características dos embeddings\n",
    "'''\n",
    "print(type(embeddings[0]))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings[0].ndim)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusteriza com UMAP e DBSCAN\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "# Referência DBSCAN: https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\n",
    "# Referênia UMAP: https://umap-learn.readthedocs.io/en/latest/\n",
    "\n",
    "## Parametriza clusterizador HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2,\n",
    "              min_samples=1,\n",
    "              metric='euclidean',\n",
    "              allow_single_cluster=False,                  \n",
    "              cluster_selection_method='eom')\n",
    "\n",
    "## Parametriza redutor UMAP\n",
    "reducer = umap.UMAP(n_neighbors=2, n_components=1, spread=0.5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "## Roda clustering com UMAP\n",
    "umap_data = reducer.fit_transform(embeddings)\n",
    "hdb = clusterer.fit(umap_data)\n",
    "## Roda clustering sem UMAP\n",
    "#hdb = clusterer.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbert\\AppData\\Local\\Temp\\ipykernel_1684\\1836898054.py:61: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  query = np.where(values == token)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Ainda no UMAP + DBSCAN, características dos clusters\n",
    "## Variáveis dos clusters\n",
    "cluster_num = clusterer.labels_.max() + 1\n",
    "cluster_labels = clusterer.labels_\n",
    "## Lista de valores - coluna 0 é o nome da chave, coluna 1 é o cluster, coluna 2 é o número de incidências, coluna 3 é o estático/variável\n",
    "values = np.empty([0,4])\n",
    "## Adicionando valores\n",
    "'''\n",
    "new_key = np.array([\"chave\",1,1,\"\"])\n",
    "new_key_2 = np.array([\"chave2\",0,4,\"\"])\n",
    "new_key_3 = np.array([\"chave\",0,4,\"\"])\n",
    "values = np.vstack((values,new_key))\n",
    "values = np.vstack((values,new_key_2))\n",
    "values = np.vstack((values,new_key_3))\n",
    "result = np.where(values == \"chave\")\n",
    "print(result)\n",
    "print(len(result))\n",
    "#is_empty = result.size == 0\n",
    "#print(is_empty)\n",
    "print(result[0])\n",
    "query = \"teste\"\n",
    "print(query)\n",
    "\n",
    "\n",
    "for i in range(len(result[0])):\n",
    "    print(int(i))\n",
    "'''\n",
    "\n",
    "## Ordenando pela coluna 2\n",
    "#values = values[values[:, 2].argsort()[::-1]]\n",
    "\n",
    "#print(cluster_num)\n",
    "#print(cluster_labels)\n",
    "'''\n",
    "query = np.where(values == \"make\")\n",
    "print(np.any(query))\n",
    "new_key = np.array([\"make\",0,1,\"\"])\n",
    "values = np.vstack((values,new_key))\n",
    "\n",
    "\n",
    "query = np.where(values == \"deploy\")\n",
    "print(np.any(query))\n",
    "new_key = np.array([\"deploy\",0,1,\"\"])\n",
    "values = np.vstack((values,new_key))\n",
    "\n",
    "query = np.where(values == \"deploy\")\n",
    "print(np.any(query))\n",
    "'''\n",
    "\n",
    "for sentence_num in range(len(sentences)):\n",
    "    sentence = sentences[sentence_num]\n",
    "    sentence_cluster = cluster_labels[sentence_num]\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    print(sentence_cluster)\n",
    "    for token in sentence_tokens:\n",
    "        #print(type(token))\n",
    "        #print(\"agora o token é {}\".format(token))\n",
    "        query = np.where(values == token)\n",
    "        #print(query)\n",
    "        #print(len(query))\n",
    "        ## Caso o token já exista no dict\n",
    "        if np.any(query):\n",
    "            #print(\"tem o token {} no dict\".format(token))\n",
    "            rows = query[0]\n",
    "            for row in rows:\n",
    "                ## Verifica se o cluster é o mesmo da frase. Se for, aumenta frequência\n",
    "                if (row == sentence_cluster):\n",
    "                    new_frequence = int(values[row][2]) + 1\n",
    "                    values[row][2] = new_frequence\n",
    "                    #print(\"aumentou frequencia\")\n",
    "                    #print(\"agora o token é {}, e aumentei a frequência dele\".format(token))\n",
    "                    break\n",
    "            else:\n",
    "                ## Se não for, é um token em um novo cluster, e fazemos sua inserção\n",
    "                new_key = np.array([token,sentence_cluster,1,\"\"])\n",
    "                values = np.vstack((values,new_key))\n",
    "        ## Se o token não existir no dict, insere-o no fim\n",
    "        else:\n",
    "            #print(\"não tem o token {} no dict\".format(token))\n",
    "            new_key = np.array([token,sentence_cluster,1,\"\"])\n",
    "            values = np.vstack((values,new_key))\n",
    "            #print(type(values[0][0]))\n",
    "            #print(\"agora o token é {}, e não achei ele na lista\".format(token))\n",
    "            #print(\"inseriu token\")\n",
    "\n",
    "#print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Token Cluster Frequence Class\n",
      "0    make       1         1      \n",
      "1  deploy       1         2      \n",
      "2     try       1         2      \n",
      "    Token Cluster Frequence Class\n",
      "0    make       1         1      \n",
      "1  deploy       1         2      \n",
      "2     try       0         2      \n"
     ]
    }
   ],
   "source": [
    "values = pd.DataFrame(columns=['Token', 'Cluster', 'Frequence', 'Class'])\n",
    "new_val = pd.DataFrame([['make', 1, 1, \"\"], ['deploy', 1, 2, \"\"], ['try', 1, 2, \"\"]], columns = ['Token', 'Cluster', 'Frequence', 'Class'])\n",
    "values = pd.concat([values,new_val], ignore_index = True)\n",
    "print(values)\n",
    "tkn = \"make\"\n",
    "tkn2 = \"Make\"\n",
    "\n",
    "query1 = values.query(\"Token == 'try'\")\n",
    "#print(query1)\n",
    "#print(len(query1))\n",
    "a = query1.index[0]\n",
    "#print(values.loc[[a]])\n",
    "\n",
    "values.at[2,'Cluster'] = 0\n",
    "\n",
    "print(values)\n",
    "query2 = values.query(\"Token == @tkn2\")\n",
    "#print(query2)\n",
    "#print(len(query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Token Cluster Frequence Class\n",
      "0    make       1         1      \n",
      "1  deploy       1         2      \n",
      "2    make       1         1      \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Ainda no UMAP + DBSCAN, características dos clusters\n",
    "## Variáveis dos clusters\n",
    "cluster_num = clusterer.labels_.max() + 1\n",
    "cluster_labels = clusterer.labels_\n",
    "\n",
    "values = pd.DataFrame(columns=['Token', 'Cluster', 'Frequence', 'Class'])\n",
    "new_val = pd.DataFrame([['make', 1, 1, \"\"], ['deploy', 1, 2, \"\"]], columns = ['Token', 'Cluster', 'Frequence', 'Class'])\n",
    "values = pd.concat([values,new_val], ignore_index = True)\n",
    "print(values)\n",
    "\n",
    "for sentence_num in range(len(sentences)):\n",
    "    sentence = sentences[sentence_num]\n",
    "    sentence_cluster = cluster_labels[sentence_num]\n",
    "    sentence_tokens = word_tokenize(sentence)\n",
    "    print(sentence_cluster)\n",
    "    for token in sentence_tokens:\n",
    "        #print(type(token))\n",
    "        #print(\"agora o token é {}\".format(token))\n",
    "        query = values.query(\"Token == @token\")\n",
    "        #print(query)\n",
    "        #print(len(query))\n",
    "        ## Caso o token já exista no dict\n",
    "        if len(query) > 0:\n",
    "            ## Verifica se o cluster é o mesmo da frase. Se for, aumenta frequência\n",
    "            if (query['Cluster'] == sentence_cluster):\n",
    "                new_frequence = int(values[row][2]) + 1\n",
    "                values[row][2] = new_frequence\n",
    "                #print(\"aumentou frequencia\")\n",
    "                #print(\"agora o token é {}, e aumentei a frequência dele\".format(token))\n",
    "                break\n",
    "            else:\n",
    "                ## Se não for, é um token em um novo cluster, e fazemos sua inserção\n",
    "                new_key = np.array([token,sentence_cluster,1,\"\"])\n",
    "                values = np.vstack((values,new_key))\n",
    "        ## Se o token não existir no dict, insere-o no fim\n",
    "        else:\n",
    "            #print(\"não tem o token {} no dict\".format(token))\n",
    "            new_key = np.array([token,sentence_cluster,1,\"\"])\n",
    "            values = np.vstack((values,new_key))\n",
    "            #print(type(values[0][0]))\n",
    "            #print(\"agora o token é {}, e não achei ele na lista\".format(token))\n",
    "            #print(\"inseriu token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "for x in range(6):\n",
    "  if x==5:\n",
    "    print(x)\n",
    "    break\n",
    "else:\n",
    "  print(\"Finally finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cluster_labels = clusterer.labels_\n",
    "print(cluster_labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgglomerativeClustering()\n",
      "[1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "## Clusteriza com SKLearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "sk_clusterer = AgglomerativeClustering().fit(embeddings)\n",
    "print(sk_clusterer)\n",
    "print(sk_clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1\n",
      "['make deploy http://www.google.com', 'make deploy http://www.microsoft.com', 'make deploy https://www.apple.com']\n",
      "\n",
      "Cluster  2\n",
      "['error http://www.google.com', 'error http://www.microsoft.com', 'error https://www.apple.com']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Clusteriza com K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Clusteriza com K-Means\n",
    "num_clusters = 2\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for _ in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Imprime labels dos clusters\n",
    "clusterer.labels_\n",
    "\n",
    "## Acha número máximo de clusters\n",
    "#clusterer.labels_.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira hipótese: usar Prefix Tree para identificar campos comuns e diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do LCS é 29\n"
     ]
    }
   ],
   "source": [
    "## Segunda hipótese: usar LCS, depois verificar tokens comuns - implementação com programação dinâmica de LCS - complexidade O(n*m)\n",
    "\n",
    "def lcs(x, y):\n",
    "  # Acha tamanho das strings\n",
    "  m = len(x)\n",
    "  n = len(y)\n",
    "\n",
    "  # Declara array para guardar os valores da programação dinâmica\n",
    "  vals = [[None]*(n + 1) for _ in range(m + 1)]\n",
    "\n",
    "  # Compara strings em ordem bottom-up\n",
    "  for i in range(m + 1):\n",
    "    for j in range(n + 1):\n",
    "      if i == 0 or j == 0 :\n",
    "        vals[i][j] = 0\n",
    "      elif x[i-1] == y[j-1]:\n",
    "        vals[i][j] = vals[i-1][j-1]+1\n",
    "      else:\n",
    "        vals[i][j] = max(vals[i-1][j], vals[i][j-1])\n",
    "\n",
    "  # vals[m][n] vai conter o tamanho do LCS entre x[0..n-1] e y[0..m-1]\n",
    "  return vals[m][n]\n",
    "\n",
    "# Teste de LCs\n",
    "print(\"O tamanho do LCS é\", lcs(log1, log3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste usando split\n",
      "['017-09-26', '12:40:15,', 'INFO', 'impl.FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home/data3/current', '30ms']\n",
      "Teste usando NLTK\n",
      "['017-09-26', '12:40:15', ',', 'INFO', 'impl.FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home/data3/current', '30ms']\n",
      "Teste usando Gensim\n",
      "['INFO', 'impl', 'FsDatasetImpl', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP', 'on', 'home', 'data', 'current', 'ms']\n",
      "Teste usando Spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['017', '-', '09', '-', '26', '12:40:15', ',', 'INFO', 'impl', '.', 'FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home', '/', 'data3', '/', 'current', '30ms']\n"
     ]
    }
   ],
   "source": [
    "## Métodos de tokenização\n",
    "\n",
    "teste = \"017-09-26 12:40:15, INFO impl.FsDatasetImpl - Time taken to scan block pool BP-805143380 on /home/data3/current 30ms\"\n",
    "\n",
    "## Teste usando split\n",
    "print (\"Teste usando split\")\n",
    "print(teste.split())\n",
    "\n",
    "## Teste usando NLTK\n",
    "print (\"Teste usando NLTK\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(teste))\n",
    "\n",
    "## Teste usando Gensim\n",
    "print (\"Teste usando Gensim\")\n",
    "from gensim.utils import tokenize\n",
    "print(list(tokenize(teste)))\n",
    "\n",
    "## Teste usando Spacy\n",
    "print (\"Teste usando Spacy\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(teste)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'deploy', 'http', ':', '//www.google.com']\n",
      "['make', 'deploy', 'http', ':', '//www.microsoft.com']\n",
      "{'make': 'STATIC', 'deploy': 'STATIC', 'http': 'STATIC', ':': 'STATIC', '//www.google.com': 'VARIABLE'}\n"
     ]
    }
   ],
   "source": [
    "## Compara duas strings para tokens similares - não leva em conta posição\n",
    "\n",
    "## Usando NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compare_strings_as_list(x,y):\n",
    "    tokens_x = word_tokenize(x)\n",
    "    tokens_y = word_tokenize(y)\n",
    "    compared = []\n",
    "    has_token = False\n",
    "\n",
    "    for i in tokens_x:\n",
    "        for j in tokens_y:\n",
    "            if i == j:\n",
    "                has_token = True\n",
    "        if has_token == True:\n",
    "            compared.append(i)\n",
    "        else:\n",
    "            compared.append(\"VARIABLE\")\n",
    "        has_token = False\n",
    "    \n",
    "    return compared\n",
    "\n",
    "#print(word_tokenize(log1))\n",
    "#print(word_tokenize(log3))\n",
    "#print(compare_strings_as_list(log1, log3))\n",
    "\n",
    "def compare_strings_as_dict(x,y):\n",
    "    tokens_x = word_tokenize(x)\n",
    "    tokens_y = word_tokenize(y)\n",
    "    compared = {}\n",
    "    has_token = False\n",
    "\n",
    "    for i in tokens_x:\n",
    "        for j in tokens_y:\n",
    "            if i == j:\n",
    "                has_token = True\n",
    "        compared[i] = \"STATIC\" if has_token == True else \"VARIABLE\"\n",
    "        has_token = False\n",
    "\n",
    "    return compared\n",
    "\n",
    "print(word_tokenize(log1))\n",
    "print(word_tokenize(log3))\n",
    "print(compare_strings_as_dict(log1, log3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
