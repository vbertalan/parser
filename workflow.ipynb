{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira etapa - receber logs\n",
    "log1 = \"make deploy http://www.google.com\"\n",
    "log2 = \"error http://www.google.com\"\n",
    "log3 = \"make deploy http://www.microsoft.com\"\n",
    "log4 = \"error http://www.microsoft.com\"\n",
    "log5 = \"make deploy https://www.apple.com\"\n",
    "log6 = \"error https://www.apple.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segunda etapa - identificar palavras em inglês? (facultativo - ver outro ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(type(embeddings[0]))\\nprint(len(embeddings[0]))\\nprint(embeddings[0].ndim)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Terceira etapa - vetorizar via Transformers\n",
    "\n",
    "## Importa Sentence Transformers e compila o modelo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "## Junta as frases\n",
    "sentences = [log1, log2, log3, log4, log5, log6]\n",
    "\n",
    "## Faz encode nas frases\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "## Printa os embeddings\n",
    "'''\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "'''\n",
    "\n",
    "## Verifica características dos embeddings\n",
    "'''\n",
    "print(type(embeddings[0]))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings[0].ndim)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusteriza com SKLearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "sk_clusterer = AgglomerativeClustering().fit(embeddings)\n",
    "print(sk_clusterer)\n",
    "print(sk_clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusteriza com K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Clusteriza com K-Means\n",
    "num_clusters = 2\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for _ in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusteriza com UMAP e DBSCAN\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "# Referência DBSCAN: https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html\n",
    "# Referênia UMAP: https://umap-learn.readthedocs.io/en/latest/\n",
    "\n",
    "## Parametriza clusterizador HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2,\n",
    "              min_samples=1,\n",
    "              metric='euclidean',\n",
    "              allow_single_cluster=False,                  \n",
    "              cluster_selection_method='eom')\n",
    "\n",
    "## Parametriza redutor UMAP\n",
    "reducer = umap.UMAP(n_neighbors=2, n_components=1, spread=0.5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "## Roda clustering com UMAP\n",
    "umap_data = reducer.fit_transform(embeddings)\n",
    "hdb = clusterer.fit(umap_data)\n",
    "## Roda clustering sem UMAP\n",
    "#hdb = clusterer.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Token Cluster Frequence Type\n",
      "0                  make       0         3     \n",
      "1                deploy       0         3     \n",
      "2                  http       0         2     \n",
      "3                     :       0         3     \n",
      "4      //www.google.com       0         1     \n",
      "5                 error       1         3     \n",
      "6                  http       1         2     \n",
      "7                     :       1         3     \n",
      "8      //www.google.com       1         1     \n",
      "9   //www.microsoft.com       0         1     \n",
      "10  //www.microsoft.com       1         1     \n",
      "11                https       0         1     \n",
      "12      //www.apple.com       0         1     \n",
      "13                https       1         1     \n",
      "14      //www.apple.com       1         1     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Pega variáveis do DBSCAN\n",
    "cluster_num = clusterer.labels_.max()\n",
    "cluster_labels = clusterer.labels_\n",
    "\n",
    "## Cria dicionário de clusters e frequências\n",
    "# sentences = list of sentences, non-vectorized\n",
    "# cluster_labels = list of cluster labels for each sentence\n",
    "def create_dict(sentences, cluster_labels):\n",
    "    \n",
    "    ## Cria dataframe\n",
    "    values = pd.DataFrame(columns=['Token', 'Cluster', 'Frequence', 'Type'])\n",
    "\n",
    "    ## Varre os tokens de cada frase\n",
    "    for sentence_num in range(len(sentences)):\n",
    "        sentence = sentences[sentence_num]\n",
    "        sentence_cluster = cluster_labels[sentence_num]\n",
    "        sentence_tokens = word_tokenize(sentence)\n",
    "        for token in sentence_tokens:\n",
    "            query = values.query(\"Token == @token\")\n",
    "            ## Caso o token já exista no dict\n",
    "            for index, result in query.iterrows():\n",
    "                ## Verifica se o cluster é o mesmo da frase. Se for, aumenta frequência\n",
    "                if (result['Cluster'] == sentence_cluster):\n",
    "                    new_frequence = result['Frequence'] + 1\n",
    "                    values.at[index,'Frequence'] = new_frequence\n",
    "                    break\n",
    "            ## Se o token não existir no dict, insere-o no fim\n",
    "            else:\n",
    "                new_val = pd.DataFrame([[token, sentence_cluster, 1, \"\"]], columns = ['Token', 'Cluster', 'Frequence', 'Type'])\n",
    "                values = pd.concat([values,new_val], ignore_index = True)\n",
    "    \n",
    "    ## Retorna dicionário de clusters e frequências\n",
    "    return values\n",
    "\n",
    "token_dict = create_dict(sentences,cluster_labels)\n",
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Token Cluster Frequence      Type\n",
      "0                  make       0         3    STATIC\n",
      "1                deploy       0         3    STATIC\n",
      "2                  http       0         2    STATIC\n",
      "3                     :       0         3    STATIC\n",
      "4      //www.google.com       0         1  VARIABLE\n",
      "5                 error       1         3    STATIC\n",
      "6                  http       1         2    STATIC\n",
      "7                     :       1         3    STATIC\n",
      "8      //www.google.com       1         1  VARIABLE\n",
      "9   //www.microsoft.com       0         1  VARIABLE\n",
      "10  //www.microsoft.com       1         1  VARIABLE\n",
      "11                https       0         1  VARIABLE\n",
      "12      //www.apple.com       0         1  VARIABLE\n",
      "13                https       1         1  VARIABLE\n",
      "14      //www.apple.com       1         1  VARIABLE\n"
     ]
    }
   ],
   "source": [
    "cluster_labels = clusterer.labels_\n",
    "\n",
    "## Define tipos de cada cluster, com base na porcentagem definida pelo usuário\n",
    "# token_dict = dicionário gerado pelo método create_dict\n",
    "# cluster_labels = list of cluster labels for each sentence\n",
    "# threshold = percentage between 0 and 1 to define static fields and variables\n",
    "def set_types(token_dict, cluster_labels, percentage):\n",
    "    \n",
    "    ## Para cada cluster, executa o código abaixo\n",
    "    for label in cluster_labels:\n",
    "        \n",
    "        ## Filtra somente os tokens com o cluster processado\n",
    "        query = token_dict.query(\"Cluster == @label\")\n",
    "        ## Acha a maior frequência do cluster\n",
    "        max_frequence = query['Frequence'].max()\n",
    "\n",
    "        ## Para cada token, verifica se a frequência é maior que a porcentagem\n",
    "        for index, result in query.iterrows():\n",
    "            current_frequence = result['Frequence']\n",
    "            current_threshold = current_frequence / max_frequence\n",
    "            ## Se for menor, é variável\n",
    "            if (current_threshold < percentage):\n",
    "                token_dict.at[index,'Type'] = \"VARIABLE\"\n",
    "            ## Se for maior, é campo estático\n",
    "            else:\n",
    "                token_dict.at[index,'Type'] = \"STATIC\"\n",
    "    \n",
    "    return token_dict\n",
    "\n",
    "print(set_types(token_dict, cluster_labels, 0.4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segunda hipótese: usar Prefix Tree para identificar campos comuns e diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do LCS é 29\n"
     ]
    }
   ],
   "source": [
    "## Terceira hipótese: usar LCS, depois verificar tokens comuns - implementação com programação dinâmica de LCS - complexidade O(n*m)\n",
    "\n",
    "def lcs(x, y):\n",
    "  # Acha tamanho das strings\n",
    "  m = len(x)\n",
    "  n = len(y)\n",
    "\n",
    "  # Declara array para guardar os valores da programação dinâmica\n",
    "  vals = [[None]*(n + 1) for _ in range(m + 1)]\n",
    "\n",
    "  # Compara strings em ordem bottom-up\n",
    "  for i in range(m + 1):\n",
    "    for j in range(n + 1):\n",
    "      if i == 0 or j == 0 :\n",
    "        vals[i][j] = 0\n",
    "      elif x[i-1] == y[j-1]:\n",
    "        vals[i][j] = vals[i-1][j-1]+1\n",
    "      else:\n",
    "        vals[i][j] = max(vals[i-1][j], vals[i][j-1])\n",
    "\n",
    "  # vals[m][n] vai conter o tamanho do LCS entre x[0..n-1] e y[0..m-1]\n",
    "  return vals[m][n]\n",
    "\n",
    "# Teste de LCs\n",
    "print(\"O tamanho do LCS é\", lcs(log1, log3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste usando split\n",
      "['017-09-26', '12:40:15,', 'INFO', 'impl.FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home/data3/current', '30ms']\n",
      "Teste usando NLTK\n",
      "['017-09-26', '12:40:15', ',', 'INFO', 'impl.FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home/data3/current', '30ms']\n",
      "Teste usando Gensim\n",
      "['INFO', 'impl', 'FsDatasetImpl', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP', 'on', 'home', 'data', 'current', 'ms']\n",
      "Teste usando Spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['017', '-', '09', '-', '26', '12:40:15', ',', 'INFO', 'impl', '.', 'FsDatasetImpl', '-', 'Time', 'taken', 'to', 'scan', 'block', 'pool', 'BP-805143380', 'on', '/home', '/', 'data3', '/', 'current', '30ms']\n"
     ]
    }
   ],
   "source": [
    "## Métodos de tokenização\n",
    "\n",
    "teste = \"017-09-26 12:40:15, INFO impl.FsDatasetImpl - Time taken to scan block pool BP-805143380 on /home/data3/current 30ms\"\n",
    "\n",
    "## Teste usando split\n",
    "print (\"Teste usando split\")\n",
    "print(teste.split())\n",
    "\n",
    "## Teste usando NLTK\n",
    "print (\"Teste usando NLTK\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(teste))\n",
    "\n",
    "## Teste usando Gensim\n",
    "print (\"Teste usando Gensim\")\n",
    "from gensim.utils import tokenize\n",
    "print(list(tokenize(teste)))\n",
    "\n",
    "## Teste usando Spacy\n",
    "print (\"Teste usando Spacy\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(teste)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'deploy', 'http', ':', '//www.google.com']\n",
      "['make', 'deploy', 'http', ':', '//www.microsoft.com']\n",
      "{'make': 'STATIC', 'deploy': 'STATIC', 'http': 'STATIC', ':': 'STATIC', '//www.google.com': 'VARIABLE'}\n"
     ]
    }
   ],
   "source": [
    "## Compara duas strings para tokens similares - não leva em conta posição\n",
    "\n",
    "## Usando NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compare_strings_as_list(x,y):\n",
    "    tokens_x = word_tokenize(x)\n",
    "    tokens_y = word_tokenize(y)\n",
    "    compared = []\n",
    "    has_token = False\n",
    "\n",
    "    for i in tokens_x:\n",
    "        for j in tokens_y:\n",
    "            if i == j:\n",
    "                has_token = True\n",
    "        if has_token == True:\n",
    "            compared.append(i)\n",
    "        else:\n",
    "            compared.append(\"VARIABLE\")\n",
    "        has_token = False\n",
    "    \n",
    "    return compared\n",
    "\n",
    "#print(word_tokenize(log1))\n",
    "#print(word_tokenize(log3))\n",
    "#print(compare_strings_as_list(log1, log3))\n",
    "\n",
    "def compare_strings_as_dict(x,y):\n",
    "    tokens_x = word_tokenize(x)\n",
    "    tokens_y = word_tokenize(y)\n",
    "    compared = {}\n",
    "    has_token = False\n",
    "\n",
    "    for i in tokens_x:\n",
    "        for j in tokens_y:\n",
    "            if i == j:\n",
    "                has_token = True\n",
    "        compared[i] = \"STATIC\" if has_token == True else \"VARIABLE\"\n",
    "        has_token = False\n",
    "\n",
    "    return compared\n",
    "\n",
    "print(word_tokenize(log1))\n",
    "print(word_tokenize(log3))\n",
    "print(compare_strings_as_dict(log1, log3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
