/var/spool/slurmd/job65522638/slurm_script: line 10: cd: /home/vberta/home/vberta/projects/def-aloise/vberta/Parser/parser: No such file or directory



huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/project/6023391/vberta/Parser/parser/.venv/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py
  warnings.warn(
Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
/project/6023391/vberta/Parser/parser/.venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2000
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 32
  Number of trainable parameters = 83504416
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: vbertalan (vbertalan88). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.1
wandb: Run data is saved locally in /project/6023391/vberta/Parser/parser/wandb/run-20230419_182850-2yfd3bta
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-aardvark-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vbertalan88/huggingface
wandb: üöÄ View run at https://wandb.ai/vbertalan88/huggingface/runs/2yfd3bta
O numero de parametros e 83504416
  0%|          | 0/32 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  3%|‚ñé         | 1/32 [00:02<01:22,  2.65s/it]  6%|‚ñã         | 2/32 [00:03<00:45,  1.53s/it]  9%|‚ñâ         | 3/32 [00:03<00:29,  1.02s/it] 12%|‚ñà‚ñé        | 4/32 [00:04<00:21,  1.28it/s] 16%|‚ñà‚ñå        | 5/32 [00:04<00:17,  1.53it/s] 19%|‚ñà‚ñâ        | 6/32 [00:05<00:14,  1.74it/s] 22%|‚ñà‚ñà‚ñè       | 7/32 [00:05<00:13,  1.90it/s] 25%|‚ñà‚ñà‚ñå       | 8/32 [00:05<00:11,  2.02it/s] 28%|‚ñà‚ñà‚ñä       | 9/32 [00:06<00:11,  1.98it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 10/32 [00:07<00:12,  1.76it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 11/32 [00:07<00:11,  1.81it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 12/32 [00:08<00:10,  1.95it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 13/32 [00:08<00:09,  2.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 14/32 [00:09<00:09,  1.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 15/32 [00:10<00:11,  1.48it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 16/32 [00:10<00:09,  1.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 17/32 [00:11<00:08,  1.74it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 18/32 [00:11<00:07,  1.82it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 19/32 [00:12<00:06,  1.96it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 20/32 [00:12<00:05,  2.06it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 21/32 [00:13<00:06,  1.78it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 22/32 [00:13<00:05,  1.75it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 23/32 [00:14<00:05,  1.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 24/32 [00:14<00:04,  1.77it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 25/32 [00:15<00:03,  1.81it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 26/32 [00:15<00:03,  1.94it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 27/32 [00:16<00:02,  1.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 28/32 [00:17<00:02,  1.61it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 29/32 [00:17<00:01,  1.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 30/32 [00:18<00:01,  1.82it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 31/32 [00:18<00:00,  1.96it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:18<00:00,  2.38it/s]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:18<00:00,  2.38it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:18<00:00,  1.69it/s]
Saving model checkpoint to ./Hadoop-Transformer
Configuration saved in ./Hadoop-Transformer/config.json
Model weights saved in ./Hadoop-Transformer/pytorch_model.bin
{'train_runtime': 30.6613, 'train_samples_per_second': 65.229, 'train_steps_per_second': 1.044, 'train_loss': 8.04930591583252, 'epoch': 1.0}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ
wandb:              train/global_step ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ
wandb:            train/train_runtime ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 32
wandb:               train/total_flos 46303449034752.0
wandb:               train/train_loss 8.04931
wandb:            train/train_runtime 30.6613
wandb: train/train_samples_per_second 65.229
wandb:   train/train_steps_per_second 1.044
wandb: 
wandb: üöÄ View run fluent-aardvark-3 at: https://wandb.ai/vbertalan88/huggingface/runs/2yfd3bta
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230419_182850-2yfd3bta/logs
